{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MI-RNN on the HAR Dataset\n",
    "\n",
    "This is an example of how the existing MI-RNN implementation can be used to train a model with shorter input sequence length on the HAR dataset. We are actively working on releasing a better implementation of both `MI-RNN` and `EMI-RNN`. This notebook only illustrates some of the features/methods we have. For instance, usage of features like embeddings, regularizers, various losses, dropout layers, various RNN cells etc are not illustrated here.\n",
    "\n",
    "Please note that, in the preprint of our work, we use the terms *bag* and *instance* to refer to the LSTM input sequence of original length and the shorter ones we want to learn to predict on, respectively. In the code though, *bag* is replaced with *instance* and *instance* is replaced with *sub-instance*. To avoid ambiguity, we will use the terms *bag* and *sub-instance*  throughout this document.\n",
    "\n",
    "The network used here is a simple LSTM + Linear classifier network. \n",
    "\n",
    "The UCI [Human Activity Recognition](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:25:40.416950Z",
     "start_time": "2018-06-19T12:25:40.403061Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "# TODO: Explain these methods\n",
    "from edgeml.mi_rnn import analysisModelMultiClass\n",
    "from edgeml.mi_rnn import NetworkV2\n",
    "from edgeml.mi_rnn import updateYPolicy4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Please download the UCI datset from the above link and use your favorite data loading methods to set up (`x_train`, `y_train`) and (`x_val`, `y_val`) numpy arrays.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "[Typical RNN models](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb) by convention, use a 3 dimensional tensor for the input data. This tensor is of shape `[number of examples, number of time steps, number of features]`. To incorporate the notion of *bags* and *sub-instances*, we extend this by adding an additional fourth dimension, thus making our input data shape - `[number of bags, number of sub-instances, number of time steps, number of features]`. Additionally, the typical shape of the one-hot encoded label tensor - `[number of examples, number of outputs]` is extended to incorporate sub-instance level labels, thus making it `[number of bags, number of sub-instances, number of output classes]`.\n",
    "\n",
    "Specifically for HAR dataset, the data creation algorithm looks something like this.\n",
    "\n",
    "```\n",
    "def createData(X, Y, subinstanceWidth, subinstanceStride):\n",
    "    '''\n",
    "    Here X and Y are time series input from HAR and their labels. This methods\n",
    "    chops the sequences into temporarily ordered set of sub-instances. All \n",
    "    sub-instances are given the same label as the bag.\n",
    "    '''\n",
    "    assert len(X) == len(Y)\n",
    "    assert len(X.shape) == 3\n",
    "    assert len(Y.shape) == 2\n",
    "    \n",
    "    X_out = []\n",
    "    Y_out = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        bag = X[i]\n",
    "        bagLabel = Y[i]\n",
    "        \n",
    "        instances = breakBagIntoInstances(bag, subinstanceWidth, subinstaceStride)\n",
    "        instanceLabels = [Y[i]] * len(instances)\n",
    "        X_out.append(instances)\n",
    "        Y_out.append(instanceLabels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:14:16.260977Z",
     "start_time": "2018-06-19T12:14:16.195740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (6220, 6, 48, 9)\n",
      "y_train shape is: (6220, 6, 6)\n",
      "x_test shape is: (1132, 6, 48, 9)\n",
      "y_test shape is: (1132, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = np.load('./HAR/48_16/x_train.npy'), np.load('./HAR/48_16/y_train.npy')\n",
    "x_test, y_test = np.load('./HAR/48_16/x_test.npy'), np.load('./HAR/48_16/y_test.npy')\n",
    "x_val, y_val = np.load('./HAR/48_16/x_val.npy'), np.load('./HAR/48_16/y_val.npy')\n",
    "\n",
    "# BAG_TEST, BAG_TRAIN, BAG_VAL are used as part of some of the analysis methods\n",
    "# These are BAG level labels.\n",
    "BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "\n",
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_val.shape)\n",
    "print(\"y_test shape is:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:14:16.283664Z",
     "start_time": "2018-06-19T12:14:16.263768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num subinstance 6\n",
      "Num time steps 48\n",
      "Num feats 9\n"
     ]
    }
   ],
   "source": [
    "# These are the parameters that are required to create the training graph\n",
    "SUBINSTANCE_WIDTH = 48\n",
    "SUBINSTANCE_STRIDE = 16\n",
    "NUM_SUBINSTANCE = x_val.shape[1]\n",
    "NUM_TIME_STEPS = x_val.shape[2]\n",
    "NUM_FEATS = x_val.shape[3]\n",
    "NUM_HIDDEN = 16\n",
    "# Even though we are using a linear layer, the linear matrix is\n",
    "# decomposed into two matrices. That is, W = W1 * W2\n",
    "# NUM_FC is the common dimension of W1 and W2. Its value is of\n",
    "# no consequence without a non-linearity or low-rank restrictions\n",
    "# and hence NUM_FC= NUM_HIDDEN  is a good default.\n",
    "NUM_FC = NUM_HIDDEN\n",
    "NUM_OUTPUT = 6\n",
    "NUM_ITER = 3\n",
    "NUM_ROUNDS = 5\n",
    "MODELDIR = '/tmp/model_dump/'\n",
    "# After each `round` of MI-RNN, the labels are updated based on a policy\n",
    "# of picking the top-k `most likely positive` elements from a bag and\n",
    "# setting the label of everything else to `noise/negative`. This happens\n",
    "# only if k >= MIN_SUBSEQUENCE_LEN\n",
    "MIN_SUBSEQUENCE_LEN = 3\n",
    "\n",
    "# Training parameters. To make sure datset API is used efficiently,\n",
    "# these parameters need to be known apriori.\n",
    "trainingParams = {\n",
    "    'batch_size': 256,\n",
    "    'max_epochs': 50,\n",
    "    'learning_rate_start': 0.001,\n",
    "}\n",
    "print('Num subinstance', NUM_SUBINSTANCE)\n",
    "print('Num time steps', NUM_TIME_STEPS)\n",
    "print('Num feats', NUM_FEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Both *MI-RNN* and *EMI-RNN* training happens in multiple *rounds*. Each round consists of two phases, the training phase where we learn the best possible model for the current information of sub-instance labels, followed by the label update phase where we use the best model we have to update the label information of the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:19:43.839243Z",
     "start_time": "2018-06-19T12:14:16.285704Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Round 0\n",
      "----------\n",
      "Iteration 0 \n",
      "Generating graph 0\n",
      "Using softmax loss\n",
      "GPU Fraction: 1.0\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.11596 Accuracy 0.94792 \n",
      "Model saved to /tmp/model_dump/, global_step 20000\n",
      "Val Accuracy: 0.961131 @ssl 2\n",
      "Iteration 1 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.10304 Accuracy 0.94336 \n",
      "Model saved to /tmp/model_dump/, global_step 20001\n",
      "Val Accuracy: 0.945230 @ssl 3\n",
      "Iteration 2 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.09217 Accuracy 0.94727 \n",
      "Model saved to /tmp/model_dump/, global_step 20002\n",
      "Val Accuracy: 0.954947 @ssl 2\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model_dump/-20000\n",
      "Restoring /tmp/model_dump/-20000\n",
      "\n",
      "Val Accuracy: 0.961131 @ssl 2\n",
      "----------\n",
      "Round 1\n",
      "----------\n",
      "Iteration 0 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.10374 Accuracy 0.94076 \n",
      "Model saved to /tmp/model_dump/, global_step 20100\n",
      "Val Accuracy: 0.948763 @ssl 3\n",
      "Iteration 1 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.09291 Accuracy 0.94792 \n",
      "Model saved to /tmp/model_dump/, global_step 20101\n",
      "Val Accuracy: 0.955830 @ssl 2\n",
      "Iteration 2 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.09105 Accuracy 0.94922 \n",
      "Model saved to /tmp/model_dump/, global_step 20102\n",
      "Val Accuracy: 0.936396 @ssl 2\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model_dump/-20101\n",
      "Restoring /tmp/model_dump/-20101\n",
      "\n",
      "Val Accuracy: 0.955830 @ssl 2\n",
      "----------\n",
      "Round 2\n",
      "----------\n",
      "Iteration 0 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.09187 Accuracy 0.94987 \n",
      "Model saved to /tmp/model_dump/, global_step 20200\n",
      "Val Accuracy: 0.936396 @ssl 3\n",
      "Iteration 1 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.07316 Accuracy 0.95508 \n",
      "Model saved to /tmp/model_dump/, global_step 20201\n",
      "Val Accuracy: 0.929329 @ssl 3\n",
      "Iteration 2 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.05490 Accuracy 0.97656 \n",
      "Model saved to /tmp/model_dump/, global_step 20202\n",
      "Val Accuracy: 0.936396 @ssl 3\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model_dump/-20200\n",
      "Restoring /tmp/model_dump/-20200\n",
      "\n",
      "Val Accuracy: 0.936396 @ssl 3\n",
      "----------\n",
      "Round 3\n",
      "----------\n",
      "Iteration 0 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.06734 Accuracy 0.97005 \n",
      "Model saved to /tmp/model_dump/, global_step 20300\n",
      "Val Accuracy: 0.930212 @ssl 3\n",
      "Iteration 1 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.07150 Accuracy 0.96159 \n",
      "Model saved to /tmp/model_dump/, global_step 20301\n",
      "Val Accuracy: 0.930212 @ssl 3\n",
      "Iteration 2 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.04770 Accuracy 0.97786 \n",
      "Model saved to /tmp/model_dump/, global_step 20302\n",
      "Val Accuracy: 0.943463 @ssl 3\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model_dump/-20302\n",
      "Restoring /tmp/model_dump/-20302\n",
      "\n",
      "Val Accuracy: 0.943463 @ssl 3\n",
      "----------\n",
      "Round 4\n",
      "----------\n",
      "Iteration 0 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.04775 Accuracy 0.97721 \n",
      "Model saved to /tmp/model_dump/, global_step 20400\n",
      "Val Accuracy: 0.945230 @ssl 2\n",
      "Iteration 1 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.04454 Accuracy 0.97331 \n",
      "Model saved to /tmp/model_dump/, global_step 20401\n",
      "Val Accuracy: 0.944346 @ssl 2\n",
      "Iteration 2 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.07998 Accuracy 0.96680 \n",
      "Model saved to /tmp/model_dump/, global_step 20402\n",
      "Val Accuracy: 0.947880 @ssl 2\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model_dump/-20402\n",
      "Restoring /tmp/model_dump/-20402\n",
      "\n",
      "Val Accuracy: 0.947880 @ssl 2\n"
     ]
    }
   ],
   "source": [
    "currentRound = 0\n",
    "reuse = False\n",
    "currY = np.array(y_train)\n",
    "while currentRound < NUM_ROUNDS:\n",
    "    print(\"%s\" %( '-' * 10))\n",
    "    print(\"Round %d\" % (currentRound))\n",
    "    print(\"%s\" %( '-' * 10))\n",
    "    globalStepBase = 20000 + currentRound * 100\n",
    "    accList = []\n",
    "    modelList = []\n",
    "    # Start training. We save a model after each interation and \n",
    "    # reload the one with the best validation set performance later\n",
    "    for i in range(NUM_ITER):\n",
    "        print(\"Iteration %d \" % (i))\n",
    "        if reuse is False:\n",
    "            print(\"Generating graph %d\" % i)\n",
    "            tf.reset_default_graph()\n",
    "            network = NetworkV2(NUM_SUBINSTANCE, NUM_FEATS,\n",
    "                                NUM_TIME_STEPS, NUM_HIDDEN,\n",
    "                                NUM_FC, NUM_OUTPUT)\n",
    "            network.createGraph(stepSize=trainingParams['learning_rate_start'])\n",
    "        \n",
    "        network.trainModel(x_train, y_train, x_val, y_val, trainingParams, reuse=reuse)\n",
    "        network.checkpointModel(MODELDIR, max_to_keep=1000,\n",
    "                                global_step = globalStepBase + i)\n",
    "        rawOut, softmaxOut, labelOut = network.inference(x_val, 50000)\n",
    "        trueLabels = np.argmax(y_val, axis=2)\n",
    "        f = open(os.devnull, 'w')\n",
    "        df = analysisModelMultiClass(labelOut, trueLabels, BAG_VAL,\n",
    "                                     NUM_SUBINSTANCE, numClass=NUM_OUTPUT,\n",
    "                                     redirFile=f)\n",
    "        f.close()\n",
    "        acc = np.max(df.acc.values)\n",
    "        # ssl: subsequence length\n",
    "        print(\"Val Accuracy: %f @ssl %d\" % (acc, np.argmax(df.acc.values) + 1))\n",
    "        accList.append(acc)\n",
    "        modelList.append((MODELDIR, globalStepBase + i))\n",
    "        reuse=True\n",
    "\n",
    "    # Load the next best model\n",
    "    idx = np.argmax(accList)\n",
    "    modelname, global_step = modelList[idx]\n",
    "    tf.reset_default_graph()\n",
    "    network = NetworkV2(NUM_SUBINSTANCE, NUM_FEATS, NUM_TIME_STEPS,\n",
    "                        NUM_HIDDEN, NUM_FC, NUM_OUTPUT, useCudnn=False)\n",
    "    graph = network.importModelTF(modelname, global_step)\n",
    "    rawOut, softmaxOut, labelOut = network.inference(x_val, 50000)\n",
    "    trueLabels = np.argmax(y_val, axis=2)\n",
    "    f = open(os.devnull, 'w')\n",
    "    df = analysisModelMultiClass(labelOut, trueLabels, BAG_VAL,\n",
    "                                NUM_SUBINSTANCE, numClass=NUM_OUTPUT, redirFile=f)\n",
    "    f.close()\n",
    "    print(\"\\nVal Accuracy: %f @ssl %d\" % (np.max(df.acc.values), np.argmax(df.acc.values) + 1))\n",
    "  \n",
    "    # Update label information\n",
    "    _, softmaxOut, _ = network.inference(x_train, 50000)\n",
    "    newY = updateYPolicy4(currY, softmaxOut, BAG_TRAIN,\n",
    "                          numClasses=NUM_OUTPUT, k=MIN_SUBSEQUENCE_LEN)\n",
    "    currY = newY\n",
    "    currentRound += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:49:35.347734Z",
     "start_time": "2018-06-18T10:49:34.870689Z"
    }
   },
   "source": [
    "## Saving and Restoring \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:19:47.868381Z",
     "start_time": "2018-06-19T12:19:43.845928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /tmp/model00_, global_step 1000\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model00_-1000\n",
      "Restoring /tmp/model00_-1000\n"
     ]
    }
   ],
   "source": [
    "network.checkpointModel('/tmp/model00_', 1000)\n",
    "tf.reset_default_graph()\n",
    "network = NetworkV2(NUM_SUBINSTANCE, NUM_FEATS, NUM_TIME_STEPS, NUM_HIDDEN, NUM_FC, NUM_OUTPUT, useCudnn=False)\n",
    "_ = network.importModelTF('/tmp/model00_', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:19:48.643613Z",
     "start_time": "2018-06-19T12:19:47.874429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   len       acc  macro-fsc  macro-pre  macro-rec  micro-fsc  micro-pre  \\\n",
      "0    1  0.893112   0.893449   0.897172   0.896078   0.893112   0.893112   \n",
      "1    2  0.908381   0.909552   0.910828   0.911163   0.908381   0.908381   \n",
      "2    3  0.913811   0.915271   0.915584   0.916341   0.913811   0.913811   \n",
      "3    4  0.895148   0.896470   0.901949   0.896227   0.895148   0.895148   \n",
      "4    5  0.875467   0.877612   0.891945   0.875583   0.875467   0.875467   \n",
      "5    6  0.857822   0.861301   0.886443   0.856575   0.857822   0.857822   \n",
      "\n",
      "   micro-rec  \n",
      "0   0.893112  \n",
      "1   0.908381  \n",
      "2   0.913811  \n",
      "3   0.895148  \n",
      "4   0.875467  \n",
      "5   0.857822  \n",
      "Max accuracy 0.913811 at subsequencelength 3\n",
      "Max micro-f 0.913811 at subsequencelength 3\n",
      "Micro-precision 0.913811 at subsequencelength 3\n",
      "Micro-recall 0.913811 at subsequencelength 3\n",
      "Max macro-f 0.915271 at subsequencelength 3\n",
      "macro-precision 0.915584 at subsequencelength 3\n",
      "macro-recall 0.916341 at subsequencelength 3\n",
      "Fraction false alarm 0.103495 (308/2976) \n"
     ]
    }
   ],
   "source": [
    "_, softmaxOut, predictions = network.inference(x_test, 1000)\n",
    "trueLabels = np.argmax(y_test, axis=2)\n",
    "bagTest = np.argmax(y_test, axis=2)[:, 0]\n",
    "df = analysisModelMultiClass(predictions, trueLabels,\n",
    "                        bagTest, NUM_SUBINSTANCE,\n",
    "                        numClass=NUM_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
