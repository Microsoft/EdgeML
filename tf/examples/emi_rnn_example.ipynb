{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using EMI-RNN on the HAR Dataset\n",
    "\n",
    "This is an example of how the existing EMI-RNN implementation can be used on the HAR dataset. We illustrate how to train a model that predicts on 48 step sequence in place of the 128 length baselines while attempting to predict early. This notebook assumes you have gone through the [mi_rnn_example](./mi_rnn_example.ipynb).\n",
    "\n",
    "Note that, we are actively working on releasing a better implementation of both `MI-RNN` and `EMI-RNN`. This notebook only illustrates some of the features/methods we have. For instance, usage of features like embeddings, regularizers, various losses, dropout layers, various RNN cells etc are not illustrated here.\n",
    "\n",
    "In the preprint of our work, we use the terms *bag* and *instance* to refer to the LSTM input sequence of original length and the shorter ones we want to learn to predict on, respectively. In the code though, *bag* is replaced with *instance* and *instance* is replaced with *sub-instance*. To avoid ambiguity, we will use the terms *bag* and *sub-instance*  throughout this document.\n",
    "\n",
    "The network used here is a simple LSTM + Linear classifier network. \n",
    "\n",
    "The UCI [Human Activity Recognition](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) dataset.\n",
    "\n",
    "## Early Classification\n",
    "\n",
    "Consider a graph with an LSTM followed by a fully connected layer. The LSTM has a hidden dimension of $l$ such that at some time $t$, the hidden state, $h_t$, and cell state, $c_t$, $\\in \\mathbb{R}^{l}$. The prediction output for a time series data point $x = [ x_1, x_2, x_3, \\cdots, x_T]$, $x_t \\in \\mathbb{R}^d$, in the *non EMI-RNN* setting is computed as follows.\n",
    "\n",
    "Let the initial LSTM states be zero initialized. That is, $h_0 = \\bf{0}$, $c_0 = \\bf{0}$. The LSTM states are updated as follows, \n",
    "\n",
    "$$\n",
    "o_t, c_t, h_t = LSTM(x_{t}, h_{t-1})\n",
    "$$\n",
    "\n",
    "and the final prediction $y_T$, is computed by passing the LSTM output at the last step $o_T$, through the linear layer, parameterized by matrices $W \\in \\mathbb{R}^{Lxh}$ and $B \\in \\mathbb{R}^{L}$, as follows\n",
    "\n",
    "$$\n",
    "\\hat{y}_T = Wo_T + B\n",
    "$$\n",
    "\n",
    "In the *EMI-RNN* algorithm, instead of only computing the prediction output $o_T$ only at the last step, we compute it at some (possibly all) intermediate steps as well. Let $S$ be the set of time steps where we compute the predictions. \n",
    "\n",
    "$$\n",
    "S \\subseteq \\{t\\ \\mid 0 \\lt t \\leq T\\}\n",
    "$$\n",
    "\n",
    "Then the *EMI-RNN* loss function becomes,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x) = \\sum_{t \\in S}(\\hat{y}_t - y)^T(\\hat{y}_t - y)\n",
    "$$\n",
    "\n",
    "Where, $y \\in \\mathbb{R}^L$ is the expected output.\n",
    "\n",
    "The set $S$, in implementation, is the `lossIndicatorList` argument to `createGraph()` method of *EMI-RNN* implementation. It defaults to computing outputs at all steps.\n",
    "\n",
    "During inference, outputs for $t \\in S$ is computed and a *evaluation policy* is used to decide whether to continue further or stop with the current prediction. A simple thresholding based evaluation policy is illustrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T10:04:59.762766Z",
     "start_time": "2018-06-20T10:04:59.741146Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sys.path.insert(0, '../')\n",
    "from edgeml.emi_rnn import NetworkJoint\n",
    "from edgeml.emi_rnn import getJointPredictions\n",
    "from edgeml.emi_rnn import earlyPolicy_baseCase\n",
    "from edgeml.emi_rnn import earlyPolicy_minProb\n",
    "from edgeml.mi_rnn import NetworkV2\n",
    "from edgeml.mi_rnn import updateYPolicy4\n",
    "from edgeml.mi_rnn import getUpdateIndexList\n",
    "from edgeml.mi_rnn import analysisModelMultiClass\n",
    "\n",
    "def getEarlySaving(predictionStep, numTimeSteps):\n",
    "    '''\n",
    "    TODO: Document/add comment\n",
    "    '''\n",
    "    predictionStep = predictionStep + 1\n",
    "    predictionStep = np.reshape(predictionStep, -1)\n",
    "    totalSteps = np.sum(predictionStep)\n",
    "    maxSteps = len(predictionStep) * numTimeSteps\n",
    "    return 1.0 - (totalSteps / maxSteps)\n",
    "\n",
    "earlyPolicy1 = earlyPolicy_baseCase\n",
    "earlyPolicy2 = earlyPolicy_minProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Please download the UCI datset from the above link and use your favorite data loading methods to set up (`x_train`, `y_train`) and (`x_val`, `y_val`) numpy arrays. The data preprocessing step is similar to that of the [mi_rnn_example](./mi_rnn_example.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T10:05:01.166773Z",
     "start_time": "2018-06-20T10:05:01.101569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (6220, 6, 48, 9)\n",
      "y_train shape is: (6220, 6, 6)\n",
      "x_test shape is: (1132, 6, 48, 9)\n",
      "y_test shape is: (1132, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = np.load('./HAR/48_16/x_train.npy'), np.load('./HAR/48_16/y_train.npy')\n",
    "x_test, y_test = np.load('./HAR/48_16/x_test.npy'), np.load('./HAR/48_16/y_test.npy')\n",
    "x_val, y_val = np.load('./HAR/48_16/x_val.npy'), np.load('./HAR/48_16/y_val.npy')\n",
    "\n",
    "# BAG_TEST, BAG_TRAIN, BAG_VAL are used as part of some of the analysis methods\n",
    "# These are BAG level labels.\n",
    "BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "\n",
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_val.shape)\n",
    "print(\"y_test shape is:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T10:05:02.679574Z",
     "start_time": "2018-06-20T10:05:02.653338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num subinstance 6\n",
      "Num time steps 48\n",
      "Num feats 9\n"
     ]
    }
   ],
   "source": [
    "SUBINSTANCE_WIDTH = 48\n",
    "SUBINSTANCE_STRIDE = 16\n",
    "NUM_SUBINSTANCE = x_val.shape[1]\n",
    "NUM_TIME_STEPS = x_val.shape[2]\n",
    "NUM_FEATS = x_val.shape[3]\n",
    "NUM_HIDDEN = 16\n",
    "# Even though we are using a linear layer, the linear matrix is\n",
    "# decomposed into two matrices. That is, W = W1 * W2\n",
    "# NUM_FC is the common dimension of W1 and W2. Its value is of\n",
    "# no consequence without a non-linearity or low-rank restrictions\n",
    "# and hence NUM_FC= NUM_HIDDEN  is a good default.\n",
    "NUM_FC = NUM_HIDDEN\n",
    "NUM_OUTPUT = 6\n",
    "NUM_ITER = 3\n",
    "NUM_ROUNDS = 5\n",
    "MODELDIR = '/tmp/model_dump/'\n",
    "# After each `round` of MI-RNN, the labels are updated based on a policy\n",
    "# of picking the top-k `most likely positive` elements from a bag and\n",
    "# setting the label of everything else to `noise/negative`. This happens\n",
    "# only if k >= MIN_SUBSEQUENCE_LEN. Please refer to the pre-print for\n",
    "# more details.\n",
    "MIN_SUBSEQUENCE_LEN = 3\n",
    "\n",
    "trainingParams = {\n",
    "    'batch_size': 256,\n",
    "    'max_epochs': 50,\n",
    "    'learning_rate_start': 0.001,\n",
    "}\n",
    "print('Num subinstance', NUM_SUBINSTANCE)\n",
    "print('Num time steps', NUM_TIME_STEPS)\n",
    "print('Num feats', NUM_FEATS)\n",
    "\n",
    "BASE_MODEL = '/tmp/model00_'\n",
    "BASE_STEP = 1000\n",
    "\n",
    "NUM_ITER = 30\n",
    "DEVNULL = open(os.devnull, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "To save us some work with *EMI-RNN*, we use a previously trained *MI-RNN* `BASE_MODEL` (trained in [mi_rnn_example](./mi_rnn_example.ipynb)) to generate the class labels. We also use the `BASE_MODEL` to initialize the *EMI-RNN* graph. The *EMI-RNN* network need not be initialized in this manner, but we have seen through our various experiments that this form of initialization leads to faster convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:26:16.834154Z",
     "start_time": "2018-06-19T12:26:10.381594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use networkv2 to update y_trian\n",
    "tf.reset_default_graph()\n",
    "network = NetworkV2(NUM_SUBINSTANCE, NUM_FEATS, NUM_TIME_STEPS,\n",
    "                    NUM_HIDDEN, NUM_FC, NUM_OUTPUT, useCudnn=False)\n",
    "graph = network.importModelTF(BASE_MODEL, BASE_STEP)\n",
    "rawOut, softmaxOut, labelOut = network.inference(x_val, 50000)\n",
    "trueLabels = np.argmax(y_val, axis=2)\n",
    "df = analysisModelMultiClass(labelOut, trueLabels, BAG_VAL, numSubinstance=NUM_SUBINSTANCE,\n",
    "                             numClass=NUM_OUTPUT, redirFile=DEVNULL)\n",
    "print(\"Val Accuracy: %f @ssl %d\" % (np.max(df.acc.values), np.argmax(df.acc.values) + 1))\n",
    "\n",
    "currY = np.array(y_train)\n",
    "_, softmaxOut, _ = network.inference(x_train, 50000)\n",
    "newY = updateYPolicy4(currY, softmaxOut, BAG_TRAIN,\n",
    "                      numClasses=NUM_OUTPUT, k=MIN_SUBSEQUENCE_LEN)\n",
    "currY = newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T12:26:21.113449Z",
     "start_time": "2018-06-19T12:26:16.836705Z"
    }
   },
   "outputs": [],
   "source": [
    "initVarList_ = network.varList\n",
    "initVarList = [network.sess.run(x) for x in initVarList_]\n",
    "network = NetworkJoint(NUM_SUBINSTANCE, NUM_FEATS, NUM_TIME_STEPS, NUM_HIDDEN, NUM_FC,\n",
    "                       NUM_OUTPUT, useCudnn=False)\n",
    "network.createGraph(stepSize=trainingParams['learning_rate_start'], initVarList = initVarList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T13:04:41.381224Z",
     "start_time": "2018-06-19T12:26:21.116234Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reuse = True\n",
    "accList = []\n",
    "for i in range(NUM_ITER):\n",
    "    network.trainModel(x_train, currY, x_val, y_val, trainingParams, reuse=reuse)\n",
    "    reuse = True\n",
    "    outputs, softmaxOut, predictions = network.inference(x_val, 1000)\n",
    "    trueLabels = np.argmax(y_val, axis=2)\n",
    "    predictions, predictionStep = getJointPredictions(softmaxOut, earlyPolicy1)\n",
    "    df = analysisModelMultiClass(predictions, trueLabels, BAG_VAL,\n",
    "                                 numSubinstance=NUM_SUBINSTANCE, numClass=NUM_OUTPUT,\n",
    "                                 verbose=False, redirFile=DEVNULL)\n",
    "    accList.append(np.max(df.acc.values))\n",
    "    print(\"Iteration %d: Val Accuracy: %f @ssl %d\" % (i, np.max(df.acc.values), np.argmax(df.acc.values) + 1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Classification\n",
    "\n",
    "We need to tune the parameters used for the early classification policy. In the simplest case, when using `earlyPolicy2`, it is just a matter of tuning the output class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T13:04:46.478437Z",
     "start_time": "2018-06-19T13:04:41.384550Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs, softmaxOut, predictions = network.inference(x_test, 1000)\n",
    "trueLabels = np.argmax(y_test, axis=2)\n",
    "minProbList = np.arange(0.95, 1.01, 0.01)\n",
    "savingsList = []\n",
    "accList = []\n",
    "for minProb in minProbList:\n",
    "    predictions, predictionStep = getJointPredictions(softmaxOut, earlyPolicy2, minProb=minProb)\n",
    "    df = analysisModelMultiClass(predictions, trueLabels, BAG_TEST,\n",
    "                                 numSubinstance=NUM_SUBINSTANCE, numClass=NUM_OUTPUT,\n",
    "                                 verbose=False, redirFile=DEVNULL)\n",
    "    savings = getEarlySaving(predictionStep, NUM_TIME_STEPS)\n",
    "    accList.append(np.max(df.acc.values))\n",
    "    savingsList.append(savings)\n",
    "\n",
    "# These are bogus figures for illustration purposes only\n",
    "baselineAcc = 0.870437\n",
    "milBestAcc = 0.877564\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "plt.plot(minProbList, savingsList, label='fraction computation saved')\n",
    "plt.plot(minProbList, accList, label='accuracy')\n",
    "plt.plot(minProbList, [baselineAcc] * len(accList), 'r--', label='baseline accuracy')\n",
    "plt.plot(minProbList, [milBestAcc] * len(accList), 'g--', label='Best acc with just MIL')\n",
    "plt.xlabel('Confidence on prediction required to preempt early')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
