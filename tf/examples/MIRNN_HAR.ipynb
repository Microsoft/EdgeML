{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MI-RNN on the HAR datset\n",
    "\n",
    "This is an example of how the existing MI-RNN code base can be used to train a model with shorter input sequence length on the HAR dataset. We are actively working on releasing a better implementation of both `MI-RNN` and `EMI-RNN`. This notebook only illustrates some of the features/methods we have. For instance, usage of features like embeddings, dropout layers, various RNN cells etc are not illustrated here.\n",
    "\n",
    "Please note that, in the preprint of our work, we use the terms *bag* and *instance* to refer to the LSTM input sequence of original length and the shorter ones we want to learn to predict on, respectively. In the code though, *bag* is replaced with *instance* and *instance* is replaced with *sub-instance*. To avoid ambiguity, I'll use the terms *bag* and *sub-instance* only.\n",
    "\n",
    "The network used here is a simple LSTM + Linear classifier network. \n",
    "\n",
    "The UCI [Human Activity Recognition](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:25:24.448765Z",
     "start_time": "2018-06-18T10:25:23.437927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-dodenn/.virtualenvs/tfsource/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "# TODO: Explain these methods\n",
    "from edgeml.graph.emi_rnn import analysisModelMultiClass\n",
    "from edgeml.graph.emi_rnn import NetworkV2\n",
    "from edgeml.graph.emi_rnn import updateYPolicy4\n",
    "from edgeml.graph.emi_rnn import getUpdateIndexList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Please download the UCI datset from the above link and use your favorite data loading methods to set up (`x_train`, `y_train`) and (`x_val`, `y_val`) numpy arrays.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "[Typical RNN models](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb) by convention, uses a 3 dimensional tensor for input with shape `[number of examples, number of time steps, number of features]`. To incorporate the notion of *bags* and *sub-instances*, we extend this by adding an additional fourth dimension, thus making our input data shape - `[number of bags, number of sub-instances, number of time steps, number of features]`. Additionally, the typical shape of the one-hot encoded label tensor - `[number of examples, number of outputs]` is extended to incorporate sub-instance level labels, thus making it `[number of bags, number of sub-instances, number of output classes]`.\n",
    "\n",
    "Specifically for HAR dataset, the data creation algorithm looks something like this.\n",
    "\n",
    "```\n",
    "def createData(X, Y, subinstanceWidth, subinstanceStride):\n",
    "    '''\n",
    "    Here X and Y are typical time series inputs and their labels. This methods\n",
    "    chops the sequences into temporarily ordered set of sub-instances. All \n",
    "    sub-instances are given the same label as the bag.\n",
    "    '''\n",
    "    assert len(X) == len(Y)\n",
    "    assert len(X.shape) == 3\n",
    "    assert len(Y.shape) == 2\n",
    "    \n",
    "    X_out = []\n",
    "    Y_out = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        bag = X[i]\n",
    "        bagLabel = Y[i]\n",
    "        \n",
    "        instances = breakBagIntoInstances(bag, subinstanceWidth, subinstaceStride)\n",
    "        instanceLabels = [Y[i]] * len(instances)\n",
    "        X_out.append(instances)\n",
    "        Y_out.append(instanceLabels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:32:11.498049Z",
     "start_time": "2018-06-18T10:32:11.416404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (6220, 6, 48, 9)\n",
      "y_train shape is: (6220, 6, 6)\n",
      "x_test shape is: (1132, 6, 48, 9)\n",
      "y_test shape is: (1132, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = np.load('./HAR/48_16/x_train.npy'), np.load('./HAR/48_16/y_train.npy')\n",
    "x_test, y_test = np.load('./HAR/48_16/x_test.npy'), np.load('./HAR/48_16/y_test.npy')\n",
    "x_val, y_val = np.load('./HAR/48_16/x_val.npy'), np.load('./HAR/48_16/y_val.npy')\n",
    "\n",
    "# BAG_TEST, BAG_TRAIN, BAG_VAL are used as part of some of the analysis methods\n",
    "# These are BAG level labels.\n",
    "BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "\n",
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_val.shape)\n",
    "print(\"y_test shape is:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:33:36.951211Z",
     "start_time": "2018-06-18T10:33:36.934134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num subinstance 6\n",
      "Num time steps 48\n",
      "Num feats 9\n"
     ]
    }
   ],
   "source": [
    "# These are the parameters that are required to create the training graph\n",
    "SUBINSTANCE_WIDTH = 48\n",
    "SUBINSTANCE_STRIDE = 16\n",
    "NUM_SUBINSTANCE = x_val.shape[1]\n",
    "NUM_TIME_STEPS = x_val.shape[2]\n",
    "NUM_FEATS = x_val.shape[3]\n",
    "NUM_HIDDEN = 16\n",
    "# TODO: Explain this. In the mean time, set it to NUM_HIDDEN\n",
    "NUM_FC = NUM_HIDDEN\n",
    "NUM_OUTPUT = 6\n",
    "NUM_ITER = 3\n",
    "NUM_ROUNDS = 5\n",
    "MODELDIR = '/tmp/model_dump/'\n",
    "MIN_SUBSEQUENCE_LEN = 3\n",
    "\n",
    "# Training parameters. To make sure datset API is used efficiently,\n",
    "# these parameters need to be known apriori.\n",
    "trainingParams = {\n",
    "    'batch_size': 256,\n",
    "    'max_epochs': 50,\n",
    "    'learning_rate_start': 0.001,\n",
    "    'keep_prob':0.85\n",
    "}\n",
    "\n",
    "print('Num subinstance', NUM_SUBINSTANCE)\n",
    "print('Num time steps', NUM_TIME_STEPS)\n",
    "print('Num feats', NUM_FEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Both *MI-RNN* and *EMI-RNN* training happens in multiple *rounds*. Each round consists two phases, the training phase where we learn the best possible model for the current information of instance labels, followed by the label update phase where we use the best model we have to update the label information of the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-18T10:33:48.619Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Round 0\n",
      "----------\n",
      "Iteration 0 \n",
      "Generating graph 0\n",
      "Using softmax loss\n",
      "GPU Fraction: 1.0\n",
      "Executing 50 epochs\n",
      "Epoch  48 Batch     0 ( 1200) Loss 0.11787 Accuracy 0.94792 \n",
      "Model saved to /tmp/model_dump/, global_step 20000\n",
      "Val Accuracy: 0.931095 @ssl 2\n",
      "Iteration 1 \n",
      "Reusing previous session\n",
      "Reusing previous init\n",
      "Executing 50 epochs\n",
      "Epoch  16 Batch    20 (  420) Loss 0.08526 Accuracy 0.95573 "
     ]
    }
   ],
   "source": [
    "currentRound = 0\n",
    "reuse = False\n",
    "currY = np.array(y_train)\n",
    "while currentRound < NUM_ROUNDS:\n",
    "    print(\"%s\" %( '-' * 10))\n",
    "    print(\"Round %d\" % (currentRound))\n",
    "    print(\"%s\" %( '-' * 10))\n",
    "    # Some random numbers as global steps so that models are not overwritten\n",
    "    globalStepBase = 20000 + currentRound * 100\n",
    "    accList = []\n",
    "    modelList = []\n",
    "    # Start training. We save a model after each interation and \n",
    "    # reload the one with the best validation set performance later\n",
    "    for i in range(NUM_ITER):\n",
    "        print(\"Iteration %d \" % (i))\n",
    "        if reuse is False:\n",
    "            print(\"Generating graph %d\" % i)\n",
    "            tf.reset_default_graph()\n",
    "            network = NetworkV2(NUM_SUBINSTANCE, NUM_FEATS,\n",
    "                                NUM_TIME_STEPS, NUM_HIDDEN,\n",
    "                                NUM_FC, NUM_OUTPUT)\n",
    "            network.createGraph(stepSize=trainingParams['learning_rate_start'])\n",
    "        \n",
    "        network.trainModel(x_train, y_train, x_val, y_val, trainingParams, reuse=reuse)\n",
    "        network.checkpointModel(MODELDIR, max_to_keep=1000,\n",
    "                                global_step = globalStepBase + i)\n",
    "        rawOut, softmaxOut, labelOut = network.inference(x_val, 50000)\n",
    "        trueLabels = np.argmax(y_val, axis=2)\n",
    "        f = open(os.devnull, 'w')\n",
    "        df = analysisModelMultiClass(labelOut, trueLabels, BAG_VAL,\n",
    "                                     NUM_SUBINSTANCE, numClass=NUM_OUTPUT,\n",
    "                                     redirFile=f)\n",
    "        f.close()\n",
    "        acc = np.max(df.acc.values)\n",
    "        # ssl: subsequence length\n",
    "        print(\"Val Accuracy: %f @ssl %d\" % (acc, np.argmax(df.acc.values) + 1))\n",
    "        accList.append(acc)\n",
    "        modelList.append((MODELDIR, globalStepBase + i))\n",
    "        reuse=True\n",
    "\n",
    "    # Load the next best model\n",
    "    idx = np.argmax(accList)\n",
    "    modelname, global_step = modelList[idx]\n",
    "    tf.reset_default_graph()\n",
    "    network = NetworkV2(NUM_SUBINSTANCE, NUM_FEATS, NUM_TIME_STEPS,\n",
    "                        NUM_HIDDEN, NUM_FC, NUM_OUTPUT, useCudnn=False)\n",
    "    graph = network.importModelTF(modelname, global_step)\n",
    "    rawOut, softmaxOut, labelOut = network.inference(x_val, 50000)\n",
    "    trueLabels = np.argmax(y_val, axis=2)\n",
    "    f = open(os.devnull, 'w')\n",
    "    df = analysisModelMultiClass(labelOut, trueLabels, BAG_VAL,\n",
    "                                NUM_SUBINSTANCE, numClass=NUM_OUTPUT, redirFile=f)\n",
    "    f.close()\n",
    "    print(\"\\nVal Accuracy: %f @ssl %d\" % (np.max(df.acc.values), np.argmax(df.acc.values) + 1))\n",
    "  \n",
    "    # Update label information\n",
    "    _, softmaxOut, _ = network.inference(x_train, 50000)\n",
    "    newY = updateYPolicy4(currY, softmaxOut, BAG_TRAIN,\n",
    "                          numClasses=NUM_OUTPUT, k=MIN_SUBSEQUENCE_LEN)\n",
    "    updateIndexBags, updateIndexTotal = getUpdateIndexList(currY, newY,\n",
    "                                        NUM_SUBINSTANCE, NUM_OUTPUT)\n",
    "    count = len(updateIndexBags)\n",
    "    print(\"Number of bag updates: %d (%f)\" % (count, count / len(newY)))\n",
    "    print(\"Number of toal updates: %d (%f)\" % (updateIndexTotal, updateIndexTotal / (len(newY) * NUM_SUBINSTANCE)))\n",
    "    currY = newY\n",
    "    currentRound += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:31:43.950259Z",
     "start_time": "2018-06-18T10:31:43.470715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /tmp/model00_, global_step 1000\n"
     ]
    }
   ],
   "source": [
    "network.checkpointModel('/tmp/model00_', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:31:47.041756Z",
     "start_time": "2018-06-18T10:31:43.952205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model00_-1000\n",
      "Restoring /tmp/model00_-1000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "network = NetworkV2(NUM_SUBINSTANCE, NUM_FEATS, NUM_TIME_STEPS, NUM_HIDDEN, NUM_FC, NUM_OUTPUT, useCudnn=False)\n",
    "_ = network.importModelTF('/tmp/model00_', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:31:47.830771Z",
     "start_time": "2018-06-18T10:31:47.047053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   len       acc  macro-fsc  macro-pre  macro-rec  micro-fsc  micro-pre  \\\n",
      "0    1  0.876824   0.878007   0.879787   0.879677   0.876824   0.876824   \n",
      "1    2  0.888022   0.889345   0.888977   0.890494   0.888022   0.888022   \n",
      "2    3  0.880556   0.881668   0.883384   0.882141   0.880556   0.880556   \n",
      "3    4  0.862233   0.863936   0.873560   0.862808   0.862233   0.862233   \n",
      "4    5  0.842891   0.846170   0.868189   0.842166   0.842891   0.842891   \n",
      "5    6  0.825585   0.830608   0.863335   0.824227   0.825585   0.825585   \n",
      "\n",
      "   micro-rec  \n",
      "0   0.876824  \n",
      "1   0.888022  \n",
      "2   0.880556  \n",
      "3   0.862233  \n",
      "4   0.842891  \n",
      "5   0.825585  \n",
      "Max accuracy 0.888022 at subsequencelength 2\n",
      "Max micro-f 0.888022 at subsequencelength 2\n",
      "Micro-precision 0.888022 at subsequencelength 2\n",
      "Micro-recall 0.888022 at subsequencelength 2\n",
      "Max macro-f 0.889345 at subsequencelength 2\n",
      "macro-precision 0.888977 at subsequencelength 2\n",
      "macro-recall 0.890494 at subsequencelength 2\n",
      "Fraction false alarm 0.038642 (115/2976) \n"
     ]
    }
   ],
   "source": [
    "_, softmaxOut, predictions = network.inference(x_test, 1000)\n",
    "trueLabels = np.argmax(y_test, axis=2)\n",
    "bagTest = np.argmax(y_test, axis=2)[:, 0]\n",
    "df = analysisModelMultiClass(predictions, trueLabels,\n",
    "                        bagTest, NUM_SUBINSTANCE,\n",
    "                        numClass=NUM_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
